{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Logistic Regression Part 2\n",
    "--------------------------\n",
    "\n",
    "In this example, we extend the code from Part 1 with several important features:\n",
    "- Instead of just updating the weight matrix ``W``, we add a bias ``b`` and use the ``.variables()`` method to compactly update both variables.\n",
    "- We attach an additional computation to the transformer to compute the loss on a held-out validation dataset.\n",
    "- We switch from a flat ``C``-dimensional feature space to a ``W x H`` feature space to demonstrate multi-dimensional logistic regression.\n",
    "\n",
    "The corresponding jupyter notebook is found [here](https://github.com/NervanaSystems/ngraph/blob/master/examples/walk_through/Logistic_Regression_Part_2.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import ngraph as ng\n",
    "import ngraph.transformers as ngt\n",
    "import gendata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The axes creation is conecptually the same as before, except we now add a new axes ``H`` to represent the new feature space. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "ax_W = ng.make_axis(length=2)\n",
    "ax_H = ng.make_axis(length=2)  # new axis added.\n",
    "ax_N = ng.make_axis(length=128, name='N')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Building the graph\n",
    "Our model, as in the previous example, has three placeholders: ``X``, ``Y``, and ``alpha``. But now, the input ``X`` has shape ``(W, H, N)``:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "alpha = ng.placeholder(())\n",
    "X = ng.placeholder([ax_W, ax_H, ax_N])  # now X has shape (W, H, N)\n",
    "Y = ng.placeholder([ax_N])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Similarly, the weight matrix is now multi-dimensional, with shape ``(W, H)``, and we add a new scalar bias variable. We want also to specify that, for the weight matrix ``W``, both axes will be reduced when computing the element-wise product and summation with the inputs (so we add ``-1`` to specify this)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "W = ng.variable([ax_W, ax_H], initial_value=0).named('W')  # now the Weight Matrix W has shape (W, H)\n",
    "b = ng.variable((), initial_value=0).named('b')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Our predicted output will now be including the bias ``b``. Please note there here the + operation implicitly broadcasts ``b`` to the batch size N, the size of the only axis of Y_hat:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "Y_hat = ng.sigmoid(ng.dot(W, X) + b)\n",
    "L = ng.cross_entropy_binary(Y_hat, Y, out_axes=()) / ng.batch_size(Y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "For the parameter updates, instead of explicitly specifying the variables ``W`` and ``b``, we can call ``L.variables()`` to retrieve all the variables that the loss function depends on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print([var.name for var in L.variables()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "For complicated ngraphs, the ``variables()`` method makes it easy to iterate over all its dependant variables. Our new parameter update is then:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "updates = [ng.assign(v, v - alpha * ng.deriv(L, v) / ng.batch_size(Y_hat))\n",
    "           for v in L.variables()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Please note that this time we embedded the (call to the) gradient computation inside the definition of the weight update computation. As stated in the previous example, the ``ng.deriv`` function computes the backprop using autodiff. The update step computes the new weight and assigns it to ``W``:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "all_updates = ng.doall(updates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Computation\n",
    "\n",
    "We have our update computation as before, but we also add an evaluation computation that computes the loss on a separate dataset without performing the updates. Since the evaluation computation does not perform any update operation, we need not pass in the learning rate ``alpha``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "transformer = ngt.make_transformer()\n",
    "\n",
    "update_fun = transformer.computation([L, W, b, all_updates], alpha, X, Y)\n",
    "eval_fun = transformer.computation(L, X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "For convenience, we define a function that computes the average cost across the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def avg_loss(xs, ys):\n",
    "    total_loss = 0\n",
    "    for x, y in zip(xs, ys):\n",
    "        loss_val = eval_fun(x, y)\n",
    "        total_loss += loss_val\n",
    "    return total_loss / x.shape[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We then generate our training and evaluation sets and perform the updates with the same technique that we used in the previous example. We emit the average loss on the validation set during training. Please note that because the length of the axes W and H is 2 now (for both; before we had only one axis of lenght 4), the number of weights is the same as in the previous example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "g = gendata.MixtureGenerator([.5, .5], (ax_W.length, ax_H.length))\n",
    "XS, YS = g.gen_data(ax_N.length, 10)\n",
    "EVAL_XS, EVAL_YS = g.gen_data(ax_N.length, 4)\n",
    "\n",
    "print(\"Starting avg loss: {}\".format(avg_loss(EVAL_XS, EVAL_YS)))\n",
    "for i in range(10):\n",
    "    for xs, ys in zip(XS, YS):\n",
    "        loss_val, w_val, b_val, _ = update_fun(5.0 / (1 + i), xs, ys)\n",
    "    print(\"After epoch %d: W: %s, b: %s, avg loss %s\" % (i, w_val.T, b_val, avg_loss(EVAL_XS, EVAL_YS)))\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
