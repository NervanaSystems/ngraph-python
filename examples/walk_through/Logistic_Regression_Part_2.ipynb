{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression Part 2\n",
    "--------------------------\n",
    "\n",
    "In this example, we extend the code from Part 1 with several important features:\n",
    "- Instead of just updating the weight matrix ``W``, we add a bias ``b`` and use the ``.variables()`` method to compactly update both variables.\n",
    "- We attach an additional computation to the transformer to compute the loss on a held-out validation dataset.\n",
    "- We switch from a flat ``C``-dimensional feature space to a ``W x H`` feature space to demonstrate multi-dimensional logistic regression.\n",
    "\n",
    "The corresponding jupyter notebook is found [here](https://github.com/NervanaSystems/ngraph/blob/master/examples/walk_through/Logistic_Regression_Part_2.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import ngraph as ng\n",
    "import ngraph.transformers as ngt\n",
    "import gendata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The axes creation is the same as before, except we now add a new axes ``H`` to represent the new feature space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ax = ng.make_name_scope(name=\"ax\")\n",
    "\n",
    "ax.W = ng.make_axis(length=4)\n",
    "ax.H = ng.make_axis(length=1)  # new axis added\n",
    "ax.N = ng.make_axis(length=128, batch=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the graph\n",
    "Our model has three placeholders: ``X``, ``Y``, and ``alpha``. Now, the the input ``X`` has shape ``(W, H, N)``:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "alpha = ng.placeholder(())\n",
    "X = ng.placeholder([ax.W, ax.H, ax.N])  # now has shape (W, H, N)\n",
    "Y = ng.placeholder([ax.N])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, the weight matrix is now multi-dimensional, with shape ``(W, H)``, and we add a new scalar bias variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W = ng.variable([ax.W - 1, ax.H - 1], initial_value=0)  # now has shape (W, H)\n",
    "b = ng.variable((), initial_value=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our predicted output now include the bias ``b``:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y_hat = ng.sigmoid(ng.dot(W, X) + b)\n",
    "L = ng.cross_entropy_binary(Y_hat, Y, out_axes=()) / ng.batch_size(Y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the parameter updates, instead of explicitly specifying the variables ``W`` and ``b``, we can call ``L.variables()`` to retrieve all the variables that the loss function depends on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print([var.name for var in L.variables()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For complicated graphs, the ``variables()`` method makes it easy to iterate over all its dependant variables. Our new parameter update is then"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "updates = [ng.assign(v, v - alpha * ng.deriv(L, v) / ng.batch_size(Y_hat))\n",
    "           for v in L.variables()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ``ng.deriv`` function computes the backprop using autodiff. We are almost done.  The update step computes the new weight and assigns it to ``W``:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_updates = ng.doall(updates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computation\n",
    "\n",
    "We have our update computation as before, but we also add an evaluation computation that computes the loss on a separate dataset without performing the updates:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "transformer = ngt.make_transformer()\n",
    "\n",
    "update_fun = transformer.computation([L, W, b, all_updates], alpha, X, Y)\n",
    "eval_fun = transformer.computation(L, X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For convenience, we define a function that computes the average cost across the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def avg_loss(xs, ys):\n",
    "    total_loss = 0\n",
    "    for x, y in zip(xs, ys):\n",
    "        loss_val = eval_fun(x, y)\n",
    "        total_loss += loss_val\n",
    "    return total_loss / x.shape[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then generate our training and evaluation sets and perform the updates. We emit the average loss on the validation set during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "g = gendata.MixtureGenerator([.5, .5], (ax.W.length, ax.H.length))\n",
    "XS, YS = g.gen_data(ax.N.length, 10)\n",
    "EVAL_XS, EVAL_YS = g.gen_data(ax.N.length, 4)\n",
    "\n",
    "print(\"Starting avg loss: {}\".format(avg_loss(EVAL_XS, EVAL_YS)))\n",
    "for i in range(10):\n",
    "    for xs, ys in zip(XS, YS):\n",
    "        loss_val, w_val, b_val, _ = update_fun(5.0 / (1 + i), xs, ys)\n",
    "    print(\"After epoch %d: W: %s, b: %s, avg loss %s\" % (i, w_val.T, b_val, avg_loss(EVAL_XS, EVAL_YS)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
