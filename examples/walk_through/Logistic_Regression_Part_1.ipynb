{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression\n",
    "-------------------\n",
    "\n",
    "This example performs logistic regression. The corresponding jupyter notebook is found [here](https://github.com/NervanaSystems/ngraph/blob/master/examples/walk_through/Logistic_Regression_Part_1.ipynb).\n",
    "We want to classify an observation $x$ into one of two classes, denoted by $y=0$ and $y=1$. Using a simple linear model:\n",
    "$$\\hat{y}=\\sigma(Wx)$$\n",
    "\n",
    "we want to find the optimal values for $W$. Here, we use gradient descent with a learning rate of $\\alpha$ and the cross-entropy as the error function.\n",
    "\n",
    "### Axes\n",
    "\n",
    "The nervana graph uses `Axes` to attach shape information to tensors. The identity of `Axis` objects are used to pair and specify dimensions in symbolic expressions. The function ``ng.make_axis`` will create an ``Axis`` object with an optionally supplied `name` argument. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import ngraph as ng\n",
    "import ngraph.transformers as ngt\n",
    "    \n",
    "my_axis = ng.make_axis(length=256).named('my_axis')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, we can use a ``NameScope`` to set the names of the various axes. A ``NameScope`` is an object that sets the name of an object to that of its assigned attribute. So when we set ``ax.N`` to an ``Axis`` object, the ``name`` of the object is automatically set to ``ax.N``. This a convenient way to define axes, so we use this approach for the rest of this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ax = ng.make_name_scope().named('ax')\n",
    "ax.N = ng.make_axis(length=128, name='N')\n",
    "ax.C = ng.make_axis(length=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We add ``batch`` as a property to ``ax.N`` to indicate that the axis is a batch axis. A batch axis is held out of the default set of axes reduced in reduction operations such as sums.\n",
    "\n",
    "### Building the graph\n",
    "Our model has three placeholders: ``X``, ``Y``, and ``alpha``, each of which need to have axes defined. ``alpha`` is a scalar, so we pass in empty axes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "alpha = ng.placeholder(axes=())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``X`` and ``Y`` are tensors for the input and output data, respectively. Our convention is to use the last axis for samples.  The placeholders can be specified as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = ng.placeholder(axes=[ax.C, ax.N])\n",
    "Y = ng.placeholder(axes=[ax.N])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to specify the training weights, ``W``.  Unlike a placeholder, ``W`` should retain its value from computation to computation (for example, across mini-batches of training). Following TensorFlow, we call this a *variable*.  We specify the variable with both ``Axes`` and also an initial value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "W = ng.variable(axes=[ax.C - 1], initial_value=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use \"dual offsets\" of +/- 1 to mark which axes should be matched during a multi-axis operation, which gives rise to the `ax.C - 1` observed above. For more information, see the `Axes` section of the user guide [here](https://ngraph.nervanasys.com/docs/latest/axes.html)\n",
    "\n",
    "Now we can estimate ``y`` as ``Y_hat`` and compute the average loss ``L``:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Y_hat = ng.sigmoid(ng.dot(W, X))\n",
    "L = ng.cross_entropy_binary(Y_hat, Y, out_axes=()) / ng.batch_size(Y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we use several ngraph functions, including ``ng.dot`` and ``ng.sigmoid``. Since a tensor can have multiple axes, we need a way to mark which axes in the first argument of ``ng.dot`` are to act on which axes in the second argument. Please also note that the `W` has been defined with one axis, while `X` has two axis. Every tensor component along C axis in `X` is being dot-producted with `W`, and the `N` results are stored in `Y_hat`, that has only one axis, the `N` axis.\n",
    "\n",
    "When ``ng.dot`` is called, it pairs axes in the first and second arguments that are of the same dual family and have consecutive positions. Keeping this logic in mind, in what is shown above, the dual position of the `X` axis of `W` was \"decreased\" to `-1`, being `0` the default position assigned when `W` was created. Remember that we want the variable `W` to act on the `ax.C` axis of the input `X`, so we want the axis for `W` to be in the position before `ax.C`, which we can obtain with `ax.C - 1`. \n",
    "\n",
    "Once `Y_hat` has been computed (the whole batch computation was defined above), we can move on and update the weights in `W`. Gradient descent requires computing the gradient, $\\frac{dL}{dW}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "grad = ng.deriv(L, W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ``ng.deriv`` function computes the backprop using autodiff. We are almost done as we are now ready to update ``W``.  The update step (which is an Op that will be carried out at the time of real computation on the device) computes the new weight and assigns it to ``W``:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "update = ng.assign(W, W - alpha * grad / ng.tensor_size(Y_hat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computation\n",
    "\n",
    "Now we create a transformer and define a computation for learning. In order to do so, we pass the ops from which we want to retrieve the results for, followed by the placeholders:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "transformer = ngt.make_transformer()\n",
    "update_fun = transformer.computation([L, W, update], alpha, X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the computation will return three values for the ``L``, ``W``, and ``update``, given inputs to fill the placeholders, $\\alpha$ (Learning Rate), X (inputs), Y (expected outputs).\n",
    "\n",
    "To run the computation we need to generate input data. Below the input data, ``X`` and ``Y``, is synthetically generated as a mixture of two Gaussian distributions in 4-d space.  We shape our entire dataset as 10 mini-batches of 128 samples each, which we create with a convenient function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import gendata\n",
    "\n",
    "g = gendata.MixtureGenerator([.5, .5], (ax.C.length,))\n",
    "XS, YS = g.gen_data(ax.N.length, 10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we train the model across the 10 epochs, printing the loss and updated weights. Please note that we are using a decreasing policy (with the epoch number) for $\\alpha$. Also note that there is no need to specify the outputs when invoking update_fun, as they were specified at definition time. Now we need only to feed the inputs into the ``update_fun`` call:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "    for i in range(10):\n",
    "        for xs, ys in zip(XS, YS):\n",
    "            loss_val, w_val, _ = update_fun(5.0 / (1 + i), xs, ys)\n",
    "            print(\"W: %s, loss %s\" % (w_val, loss_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also see Part 2 of logistic regressions, which walks users through adding additional variables, computations, and dimensions. <br>  \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
