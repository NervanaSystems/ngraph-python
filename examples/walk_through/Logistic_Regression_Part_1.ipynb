{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression\n",
    "-------------------\n",
    "\n",
    "This example performs logistic regression. The corresponding jupyter notebook is found [here](https://github.com/NervanaSystems/ngraph/blob/master/examples/walk_through/Logistic_Regression_Part_1.ipynb).\n",
    "\n",
    "We want to classify an observation $x$ into one of two classes, denoted by $y=0$ and $y=1$. Using a simple linear model:\n",
    "$$\\hat{y}=\\sigma(Wx)$$\n",
    "\n",
    "we want to find the optimal values for $W$. Here, we use gradient descent with a learning rate of $\\alpha$ and the cross-entropy as the error function.\n",
    "\n",
    "### Axes\n",
    "\n",
    "The nervana graph uses `Axes` to attach shape information to tensors. The identity of `Axis` objects are used to pair and specify dimensions in symbolic expressions. The function ``ng.make_axis`` will create an ``Axis`` object with an optionally supplied `name` argument. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import ngraph as ng\n",
    "import ngraph.transformers as ngt\n",
    "    \n",
    "my_axis = ng.make_axis(length=256, name='my_axis')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, we can use a ``NameScope`` to set the names of the various axes. A ``NameScope`` is an object that sets the name of an object to that of its assigned attribute. So when we set ``ax.N`` to an ``Axis`` object, the ``name`` of the object is automatically set to ``ax.N``. This a convenient way to define axes, so we use this approach for the rest of this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ax = ng.make_name_scope(\"ax\")\n",
    "ax.N = ng.make_axis(length=128, batch=True)\n",
    "ax.C = ng.make_axis(length=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We add ``batch`` as a property to ``ax.N`` to indicate that the axis is a batch axis. A batch axis is held out of the default set of axes reduced in reduction operations such as sums.\n",
    "\n",
    "### Building the graph\n",
    "Our model has three placeholders: ``X``, ``Y``, and ``alpha``, each of which need to have axes defined. ``alpha`` is a scalar, so we pass in empty axes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "alpha = ng.placeholder(axes=())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``X`` and ``Y`` are tensors for the input and output data, respectively. Our convention is to use the last axis for samples.  The placeholders can be specified as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = ng.placeholder(axes=[ax.C, ax.N])\n",
    "Y = ng.placeholder(axes=[ax.N])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to specify the training weights, ``W``.  Unlike a placeholder, ``W`` should retain its value from computation to computation (for example, across mini-batches of training).  Following TensorFlow, we call this a *variable*.  We specify the variable with both ``Axes`` and also an initial value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W = ng.variable(axes=[ax.C - 1], initial_value=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The nervana graph axes are agnostic to data layout on the compute device, so the ordering of the axes does not matter. As a consequence, when two tensors are provided to a `ng.dot()` operation, for example, one needs to indicate which are the corresponding axes that should be matched together. We use \"dual offsets\" of +/- 1 to mark which axes should be matched during a multi-axis operation, which gives rise to the `ax.C - 1` observed above. For more information, see the `Axes` section of the user guide.\n",
    "\n",
    "Now we can estimate ``y`` as ``Y_hat`` and compute the average loss ``L``:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Y_hat = ng.sigmoid(ng.dot(W, X))\n",
    "L = ng.cross_entropy_binary(Y_hat, Y, out_axes=()) / ng.batch_size(Y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we use several ngraph functions, including ``ng.dot`` and ``ng.sigmoid``. Since a tensor can have multiple axes, we need a way to mark which axes in the first argument of ``ng.dot`` are to act on which axes in the second argument.\n",
    "\n",
    "Every axis is a member of a family of axes we call duals of the axis, and each axis in the family has a position. When you create an axis, its dual position is 0. ``dot`` pairs axes in the first and second arguments that are of the same dual family and have consecutive positions.\n",
    "\n",
    "We want the variable `W` to act on the `ax.C` axis, so we want the axis for `W` to be in the position before `ax.C`, which we can obtain with `ax.C - 1`. We initialize ``W`` to ``0``.\n",
    "\n",
    "Gradient descent requires computing the gradient, $\\frac{dL}{dW}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grad = ng.deriv(L, W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ``ng.deriv`` function computes the backprop using autodiff. We are almost done.  The update step computes the new weight and assigns it to ``W``:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "update = ng.assign(W, W - alpha * grad / ng.tensor_size(Y_hat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computation\n",
    "\n",
    "Now we create a transformer and define a computation. We pass the ops from which we want to retrieve the results for, followed by the placeholders:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ngt.make_transformer()\n",
    "\n",
    "transformer = ngt.make_transformer()\n",
    "update_fun = transformer.computation([L, W, update], alpha, X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the computation will return three values for the ``L``, ``W``, and ``update``, given inputs to fill the placeholders. \n",
    "\n",
    "The input data is synthetically generated as a mixture of two Gaussian distributions in 4-d space.  Our dataset consists of 10 mini-batches of 128 samples each, which we create with a convenience function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import gendata\n",
    "\n",
    "g = gendata.MixtureGenerator([.5, .5], (ax.C.length,))\n",
    "XS, YS = g.gen_data(ax.N.length, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we train the model across 10 epochs, printing the loss and updated weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "    for i in range(10):\n",
    "        for xs, ys in zip(XS, YS):\n",
    "            loss_val, w_val, _ = update_fun(5.0 / (1 + i), xs, ys)\n",
    "            print(\"W: %s, loss %s\" % (w_val, loss_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also see Part 2 of logistic regressions, which walks uses through adding additional variables, computations, and dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
